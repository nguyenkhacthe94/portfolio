<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Database Optimization: From 45 Minutes to 20 Seconds - The Nguyen Khac</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=Lora:wght@400;500;600;700&family=DM+Sans:wght@400;500;700&display=swap"
    rel="stylesheet">
  <link rel="stylesheet" href="../../css/posts.css">
</head>

<body>
  <div id="nav-placeholder"></div>

  <div class="container">
    <article>
      <!-- Article Header -->
      <header class="article-header">
        <div class="article-meta">Technical Article • Database Optimization</div>
        <h1>DATABASE SPEED INCREASED 25 TIMES with just a few simple commands</h1>
        <img src="../../img/db_optimization_thumbnail.png" alt="Database Optimization Thumbnail" class="article-thumbnail">
        <p class="article-intro">
          It sounds dramatic, but it's genuinely simple. The truth is that mistakes like this can happen anywhere.
          Database optimization is something businesses highly value. Give this a read—you might find it useful
          somewhere.
        </p>
      </header>

      <!-- Article Content -->
      <div class="article-content">
        <h2>Background</h2>
        <p>
          I took on a project with quite a large dataset. Our system uses <strong>SQL Server Enterprise 2016</strong>
          with the client being <strong>SQL Server Management Studio 2020</strong>. The data comprises the sales figures
          of <strong>ALL electronic products</strong>, from <strong>ALL brands</strong>, in <strong>ALL
            countries/regions</strong>, for <strong>EACH MONTH</strong> from 2007 to now. With such a dataset, slow
          queries were <strong>INEVITABLE</strong>. Here's the situation:
        </p>

        <pre><code>SELECT * FROM &lt;TABLE&gt;
WHERE 1 = 1
AND NationCode = 'XX'
AND ProductCode = 'YY' 
AND Period like '20xx%'</code></pre>

        <p>
          The query looks simple, but it took about <strong>25–30 minutes</strong> to get results. For more complex
          nested queries with ordering for report screens, it took about <strong>45 minutes</strong> to process.
        </p>

        <p>
          Looking at databases in other projects, I saw many larger databases running much faster. That's when I
          realized something was wrong with my database. So, I started investigating.
        </p>

        <hr>

        <h2>I. INVESTIGATION</h2>

        <h3>1. Checking Database Settings</h3>
        <p>
          After tracing the report screens, I found stored procedures corresponding to each screen. Each procedure had a
          simple query with 3 parameters like the one mentioned above. Since the query was simple, I didn't look there
          for the issue. Instead, I checked the database settings (under Options in Database Properties):
        </p>

        <ul>
          <li><strong>Auto Create Statistics</strong>: True</li>
          <li><strong>Auto Update Statistics</strong>: True</li>
          <li><strong>Auto Shrink</strong>: False</li>
        </ul>

        <p>
          The Auto Shrink setting indicated potential fragmentation issues. I focused on 4 key tables that fed data to
          the report screens—the critical tables of the system.
        </p>

        <h3>2. Checking Tables Structure</h3>
        <ul>
          <li>No indexes on search fields. Even when indexed, it wasn't effective due to excessive duplication in the
            search fields.</li>
          <li>No partitioning, as old data was frequently deleted or updated.</li>
          <li>Used clustered columnstore indexes for numeric calculations.</li>
          <li>Tables were compressed using columnstore indexes.</li>
        </ul>

        <p>
          The table design seemed fine, but since Auto Shrink wasn't enabled, I decided to check for fragmentation.
        </p>

        <h3>3. Checking Fragmentation</h3>
        <p>I used the following command to check:</p>

        <pre><code>DBCC SHOWCONTIG ('TABLE_NAME')</code></pre>

        <p><strong>Results:</strong></p>

        <table>
          <thead>
            <tr>
              <th>Metric</th>
              <th>Value</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Pages Scanned</td>
              <td>6,196,802</td>
            </tr>
            <tr>
              <td>Extents Scanned</td>
              <td>779,420</td>
            </tr>
            <tr>
              <td>Extent Switches</td>
              <td>2,000,302</td>
            </tr>
            <tr>
              <td>Avg. Pages per Extent</td>
              <td>8.0</td>
            </tr>
            <tr>
              <td><strong>Scan Density</strong></td>
              <td><strong>38.72%</strong></td>
            </tr>
            <tr>
              <td><strong>Logical Scan Fragmentation</strong></td>
              <td><strong>20.82%</strong></td>
            </tr>
            <tr>
              <td><strong>Extent Scan Fragmentation</strong></td>
              <td><strong>50.15%</strong></td>
            </tr>
            <tr>
              <td>Avg. Bytes Free per Page</td>
              <td>754.8</td>
            </tr>
            <tr>
              <td>Avg. Page Density</td>
              <td>90.67%</td>
            </tr>
          </tbody>
        </table>

        <p>
          The three fragmentation metrics were <strong>ALARMINGLY HIGH</strong>. Essentially, SQL Server was scanning
          far too many underfilled pages, leading to inefficiencies.
        </p>

        <p>
          <strong>Diagnosis:</strong> The critical tables were <strong>SEVERELY FRAGMENTED</strong>.
        </p>

        <hr>

        <h2>II. TESTING</h2>
        <p>
          This alone wasn't enough to convince management, who believed the system was unfixable. I needed a test
          environment to prove my point. Luckily, I had a server to test on (same SQL Server version).
        </p>

        <p>I replicated the tables with the same structure and imported the entire dataset.</p>

        <h3>1. Observation</h3>
        <p>
          On the test server, the data only occupied <strong>29GB</strong>, compared to <strong>360GB</strong> in
          production—a <strong>12x reduction</strong>.
        </p>

        <h3>2. Query performance comparison:</h3>
        <ul>
          <li><strong>Production:</strong> 21:28</li>
          <li><strong>Test:</strong> 2:51</li>
        </ul>

        <h3>3. Data density comparison:</h3>
        <ul>
          <li><strong>Production:</strong> 38.72%</li>
          <li><strong>Test:</strong> 97.56%</li>
        </ul>

        <p>
          These results, along with disk usage metrics, formed the basis for my proposal to the manager.
        </p>

        <hr>

        <h2>III. IMPLEMENTATION</h2>

        <h3>1. Preparation</h3>
        <ul>
          <li>Suspend all data modifications (insert/update/delete).</li>
          <li>Back up data according to the <strong>3-2-1-0-0 principle</strong>: full database snapshot, and table
            backups on both the dev and production servers.</li>
          <li>Schedule the operation for the weekend to minimize client impact.</li>
          <li>Notify clients about the downtime.</li>
        </ul>

        <h3>2. Execution</h3>
        <p>Truncate the tables, reload the data, and validate at every step:</p>

        <ol>
          <li>Load raw data.</li>
          <li>Verify report screens.</li>
          <li>Check data exports to other systems.</li>
        </ol>

        <p>If any issue arose, the data would need to be restored (thankfully, there were no problems).</p>

        <hr>

        <h2>IV. RESULTS</h2>

        <table>
          <thead>
            <tr>
              <th>Metric</th>
              <th>Before</th>
              <th>After</th>
              <th>Improvement</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Query Time</strong></td>
              <td>21:28</td>
              <td>0:20</td>
              <td><strong>60x faster</strong></td>
            </tr>
            <tr>
              <td><strong>Disk Usage</strong></td>
              <td>360 GB</td>
              <td>29 GB</td>
              <td><strong>92% reduction</strong></td>
            </tr>
            <tr>
              <td><strong>Scan Density</strong></td>
              <td>38.72%</td>
              <td>97.56%</td>
              <td><strong>2.5x improvement</strong></td>
            </tr>
          </tbody>
        </table>

        <img src="../../img/db_optimization_result.png" alt="Database Optimization Results">
        <p class="image-caption">Visual comparison of query performance before and after optimization</p>

        <hr>

        <h2>V. CONCLUSION</h2>
        <p>The key takeaway here is:</p>

        <ol>
          <li><strong>Finding the RIGHT issue to fix is crucial.</strong></li>
          <li><strong>Even small actions can drastically improve system performance.</strong></li>
          <li><strong>No matter how experienced someone is, always validate your assumptions.</strong></li>
        </ol>
      </div>
    </article>
  </div>

  <div id="footer-placeholder"></div>

  <script src="../../js/post_layout.js"></script>
</body>

</html>
